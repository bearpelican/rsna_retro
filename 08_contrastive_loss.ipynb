{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading imports\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from rsna_retro.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss \n",
    "\n",
    "### Taken from here: https://github.com/adambielski/siamese-triplet/blob/master/losses.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss\n",
    "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative=None, size_average=True):\n",
    "        if negative is None: negative = positive.flip(dims=[0])\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checking Triple loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tloss = TripletLoss(margin=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(10.6520))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anch = torch.randn(4, 8) # bs x features\n",
    "pos = anch + 0.1 # bs x feat\n",
    "neg = pos.flip(dims=[0])\n",
    "tloss(anch,pos,neg), tloss(anch,neg,pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tloss(anch,pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# https://github.com/adambielski/siamese-triplet/blob/master/losses.py\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss\n",
    "    Takes embeddings of two samples and a target label == 1 if samples are from the same class and label == 0 otherwise\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, output1, output2, target, size_average=True):\n",
    "        distances = (output2 - output1).pow(2).sum(-1)  # squared distances\n",
    "#         distances = F.pairwise_distance(output1, output2, keepdim=True).pow(2).sum(-1)  # squared distances\n",
    "        losses = (target.float() * distances +\n",
    "                        (1 + -1 * target).float() * F.relu(self.margin - (distances + self.eps).sqrt()).pow(2))\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code\n",
    "# https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch/blob/master/Siamese-networks-medium.ipynb\n",
    "class PairContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = False)\n",
    "        if len(euclidean_distance.shape)>1: euclidean_distance = euclidean_distance.sum(-1)\n",
    "        c_pos = (label) * torch.pow(euclidean_distance, 2)\n",
    "        c_neg = (1-label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        loss_contrastive = (c_neg + c_pos)\n",
    "        return loss_contrastive.mean()\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# https://stackoverflow.com/questions/47107589/how-do-i-use-a-bytetensor-in-a-contrastive-cosine-loss-function\n",
    "class CosineContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        cos_sim = F.cosine_similarity(output1, output2, dim=-1)\n",
    "        loss_cos_con = torch.mean((label) * torch.div(torch.pow((1.0-cos_sim), 2), 4) +\n",
    "                                    (1-label) * torch.pow(cos_sim * torch.lt(cos_sim, self.margin), 2))\n",
    "        return loss_cos_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(output1, output2):\n",
    "    num = output1.T @ output2\n",
    "    denom = torch.norm(output1) * torch.norm(output2)\n",
    "    out = num / denom\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = {\n",
    "    '1_contloss': ContrastiveLoss(),\n",
    "    '2_pairloss': PairContrastiveLoss(),\n",
    "    '3_cosloss': CosineContrastiveLoss(),\n",
    "#     '4_xentloss': XentContrastiveLoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_out(t,a,l):\n",
    "    return {k:v(t,a,l) for k,v in ls.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8]), torch.Size([4, 8]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ = torch.randn(4, 8) # bs x features\n",
    "aug = targ + 0.1 # bs x feat\n",
    "targ.shape, aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_contloss': tensor(0.0200),\n",
       " '2_pairloss': tensor(0.0200),\n",
       " '3_cosloss': tensor(0.0552)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.zeros(aug.shape[0])\n",
    "labels[0] = 1\n",
    "\n",
    "# single target to rest of batch\n",
    "loss_out(targ[:1], aug, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_contloss': tensor(0.0800),\n",
       " '2_pairloss': tensor(0.0800),\n",
       " '3_cosloss': tensor(1.8311e-05)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-to-1 match targ -> aug. Loss should be 8x higher\n",
    "labels = torch.ones(aug.shape[0])\n",
    "loss_out(targ, aug, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_contloss': tensor(11.8284),\n",
       " '2_pairloss': tensor(11.8284),\n",
       " '3_cosloss': tensor(0.2118)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single targ -> all rand. Super High Loss\n",
    "labels = torch.ones(aug.shape[0])\n",
    "loss_out(targ[:1], aug, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batched contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def batched_labels(output1, output2, onehot=True):\n",
    "    bs = output1.shape[0]\n",
    "    rp = [1]*len(output1.shape)\n",
    "    o1 = output1.repeat(*rp,bs).view(bs,*output1.shape)\n",
    "    labels = torch.arange(bs, device=output1.device)\n",
    "    if onehot: labels = torch.eye(o1.shape[0], device=output1.device)[labels]\n",
    "    return o1, output2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def batched_labels(output1, output2, onehot=True):\n",
    "    bs,feat = output1.shape\n",
    "    o1 = output1.view(bs,1,feat)\n",
    "    labels = torch.arange(bs, device=output1.device)\n",
    "    if onehot: labels = torch.eye(o1.shape[0], device=output1.device)[labels]\n",
    "    return o1, output2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "closs = ContrastiveLoss()\n",
    "# closs = CosineContrastiveLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,feat=6,18 # \n",
    "targ = torch.randn(bs, feat)*5 # bs x features\n",
    "aug = targ+0.1 # bs x features\n",
    "# aug = torch.randn(bs, feat) # bs x features\n",
    "o1, o2, l = batched_labels(targ, aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 18])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1464)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = F.cosine_similarity(o1, o2, dim=-1)\n",
    "F.cross_entropy(cs, torch.arange(bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = []\n",
    "for i in range(targ.shape[0]):\n",
    "    bs = targ.shape[0]\n",
    "    labels = torch.zeros(bs)\n",
    "    labels[i] = 1 # set current target as the only positive label\n",
    "    losses.append(closs(targ[i], aug, labels))\n",
    "torch.stack(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closs(o1,o2,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(118.5505)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checking - setting wrong target as positive label\n",
    "losses = []\n",
    "for i in range(targ.shape[0]):\n",
    "    bs = targ.shape[0]\n",
    "    labels = torch.zeros(bs)\n",
    "    labels[0] = 1 # set current target as the only positive label\n",
    "    losses.append(closs(targ[i], aug, labels))\n",
    "torch.stack(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(118.5505)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l3 = l[torch.zeros(o1.shape[0]).long()]\n",
    "closs(o1,o2,l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(767.6289)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checking - setting wrong target as positive label\n",
    "losses = []\n",
    "for i in range(targ.shape[0]):\n",
    "    bs = targ.shape[0]\n",
    "    labels = torch.ones(bs)\n",
    "    labels[0] = 1 # set current target as the only positive label\n",
    "    losses.append(closs(targ[i], aug, labels))\n",
    "torch.stack(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(767.6289)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l4 = torch.ones(o1.shape[0], o1.shape[0])\n",
    "closs(o1,o2,l4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XentContrastiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# https://arxiv.org/pdf/2002.05709.pdf\n",
    "class XentOldContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temp=0.5):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "\n",
    "    def forward(self, output1, output2, labels):\n",
    "        cos_sim = F.cosine_similarity(output1, output2, dim=-1)/self.temp\n",
    "        xent_loss = F.cross_entropy(cos_sim,labels.long())\n",
    "        return xent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,feat=4,16 # \n",
    "targ = torch.randn(bs, feat) # bs x features\n",
    "rand_aug = torch.randn(bs, feat) # bs x features\n",
    "aug = targ + 0.1 # bs x feat\n",
    "output1, output2, labels = batched_labels(targ, aug, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7361)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, lmax = labels.max(dim=-1); lmax\n",
    "xent_loss = XentOldContrastiveLoss(1.0)\n",
    "xent_loss(output1, output2, lmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = F.cosine_similarity(output1, output2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7000, -2.2669, -1.4111, -1.8586],\n",
       "        [-2.2112, -0.6351, -1.8216, -1.6156],\n",
       "        [-1.5602, -2.0221, -0.8183, -1.5310],\n",
       "        [-1.9654, -1.7637, -1.4475, -0.7911]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsoft = F.log_softmax(cos_sim, dim=-1); lsoft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7000, -2.1996, -1.5289, -1.9389],\n",
       "        [-2.2785, -0.6351, -2.0068, -1.7632],\n",
       "        [-1.4423, -1.8369, -0.8183, -1.4934],\n",
       "        [-1.8852, -1.6161, -1.4851, -0.7911]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsoft1 = torch.log(torch.exp(cos_sim)/torch.exp(cos_sim).sum(dim=-1)); lsoft1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7361)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nll loss\n",
    "l_nll = torch.mean(torch.sum(-labels * lsoft1, dim=1)); l_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7361)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.nll_loss(lsoft, lmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Xent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9973, -0.5696,  0.2862, -0.1613],\n",
       "        [-0.5812,  0.9949, -0.1916,  0.0144],\n",
       "        [ 0.2550, -0.2069,  0.9969,  0.2842],\n",
       "        [-0.1879,  0.0139,  0.3300,  0.9865]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7110, 0.5657, 1.3314, 0.8510],\n",
       "        [0.5592, 2.7045, 0.8256, 1.0145],\n",
       "        [1.2905, 0.8131, 2.7098, 1.3287],\n",
       "        [0.8287, 1.0140, 1.3910, 2.6817]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cexp = torch.exp(cos_sim)\n",
    "cexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.4592, 5.1039, 6.1420, 5.9154])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cexp.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7481, 2.3994, 3.4322, 3.2337])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_denom = (cexp*(1-labels)).sum(dim=-1); neg_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9973, 0.9949, 0.9969, 0.9865])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = cos_sim[range(lmax.shape[0]),lmax]; pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7481, 2.3994, 3.4322, 3.2337])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cexp.sum(dim=-1) - torch.exp(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0136, -1.4448, -0.9470, -1.3350],\n",
       "        [-1.5921,  0.1197, -1.4248, -1.1592],\n",
       "        [-0.7559, -1.0821, -0.2363, -0.8895],\n",
       "        [-1.1988, -0.8613, -0.9032, -0.1872]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsoft2 = torch.log(cexp/neg_denom); lsoft2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2509)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_nll = torch.mean(torch.sum(-lmax * lsoft2, dim=1)); l_nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7110, 0.5657, 1.3314, 0.8510],\n",
       "        [0.5592, 2.7045, 0.8256, 1.0145],\n",
       "        [1.2905, 0.8131, 2.7098, 1.3287],\n",
       "        [0.8287, 1.0140, 1.3910, 2.6817]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = 1.0\n",
    "cos_sim = F.cosine_similarity(output1, output2, dim=-1)/temp\n",
    "cexp = torch.exp(cos_sim)\n",
    "cexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7110, 2.7045, 2.7098, 2.6817])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (cexp * labels).sum(dim=-1); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7481, 2.3994, 3.4322, 3.2337])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cexp*(1-labels)).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7481, 2.3994, 3.4322, 3.2337])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denom = cexp.sum(dim=-1) - x; denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0136,  1.4448,  0.9470,  1.3350],\n",
       "        [ 1.5921, -0.1197,  1.4248,  1.1592],\n",
       "        [ 0.7559,  1.0821,  0.2363,  0.8895],\n",
       "        [ 1.1988,  0.8613,  0.9032,  0.1872]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsoft = -torch.log(cexp/denom)\n",
    "lsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0136, -0.1197,  0.2363,  0.1872])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(x)+torch.log(denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0136, -0.1197,  0.2363,  0.1872])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_denom = (cexp*(1-labels)).sum(dim=-1); neg_denom\n",
    "lsoft1 = torch.log(cexp/neg_denom)\n",
    "lsoft2 = torch.sum(-labels * lsoft1, dim=-1)\n",
    "lsoft2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sim_mat(x, y=None):\n",
    "    \"\"\"\n",
    "    returns a matrix where entry (i,j) is the dot product of x[i] and x[j]\n",
    "    \"\"\"\n",
    "    if y is None:\n",
    "        y = x\n",
    "    return torch.matmul(x, y.t())\n",
    "\n",
    "def convert_to_pairs(indices_tuple, labels):\n",
    "    \"\"\"\n",
    "    This returns anchor-positive and anchor-negative indices,\n",
    "    regardless of what the input indices_tuple is\n",
    "    Args:\n",
    "        indices_tuple: tuple of tensors. Each tensor is 1d and specifies indices\n",
    "                        within a batch\n",
    "        labels: a tensor which has the label for each element in a batch\n",
    "    \"\"\"\n",
    "    if indices_tuple is None:\n",
    "        return get_all_pairs_indices(labels)\n",
    "    elif len(indices_tuple) == 4:\n",
    "        return indices_tuple\n",
    "    else:\n",
    "        a, p, n = indices_tuple\n",
    "        return a, p, a, n\n",
    "\n",
    "def get_all_pairs_indices(labels, ref_labels=None):\n",
    "    \"\"\"\n",
    "    Given a tensor of labels, this will return 4 tensors.\n",
    "    The first 2 tensors are the indices which form all positive pairs\n",
    "    The second 2 tensors are the indices which form all negative pairs\n",
    "    \"\"\"\n",
    "    if ref_labels is None:\n",
    "        ref_labels = labels\n",
    "    labels1 = labels.unsqueeze(1)\n",
    "    labels2 = ref_labels.unsqueeze(0)\n",
    "    matches = (labels1 == labels2).byte()\n",
    "    diffs = matches ^ 1\n",
    "    if ref_labels is labels:\n",
    "        matches -= torch.eye(matches.size(0)).byte().to(labels.device)\n",
    "    a1_idx = matches.nonzero()[:, 0].flatten()\n",
    "    p_idx = matches.nonzero()[:, 1].flatten()\n",
    "    a2_idx = diffs.nonzero()[:, 0].flatten()\n",
    "    n_idx = diffs.nonzero()[:, 1].flatten()\n",
    "    return a1_idx, p_idx, a2_idx, n_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTXentLoss2(nn.Module):\n",
    "\n",
    "    def __init__(self, temperature, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.temperature = temperature\n",
    "        self.normalize_embeddings = True\n",
    "\n",
    "    def forward(self, embeddings, labels, indices_tuple):\n",
    "        cosine_similarity = sim_mat(embeddings)\n",
    "        if not self.normalize_embeddings:\n",
    "            embedding_norms_mat = self.embedding_norms.unsqueeze(0)*self.embedding_norms.unsqueeze(1)\n",
    "            cosine_similarity = cosine_similarity / (embedding_norms_mat)\n",
    "        cosine_similarity = cosine_similarity / self.temperature\n",
    "\n",
    "        a1, p, a2, n = convert_to_pairs(indices_tuple, labels)\n",
    "\n",
    "        if len(a1) > 0 and len(a2) > 0:\n",
    "            pos_pairs = cosine_similarity[a1, p].unsqueeze(1)\n",
    "            neg_pairs = cosine_similarity[a2, n]\n",
    "            n_per_p = (a2.unsqueeze(0) == a1.unsqueeze(1)).float()\n",
    "            neg_pairs = neg_pairs*n_per_p\n",
    "            neg_pairs[n_per_p==0] = float('-inf')\n",
    "\n",
    "            max_val = torch.max(pos_pairs, torch.max(neg_pairs, dim=1, keepdim=True)[0])\n",
    "            numerator = torch.exp(pos_pairs - max_val).squeeze(1)\n",
    "            denominator = torch.sum(torch.exp(neg_pairs - max_val), dim=1) + numerator\n",
    "            log_exp = torch.log((numerator/denominator) + 1e-20)\n",
    "            return torch.mean(-log_exp)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'NTXentLoss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-9e98ec61d774>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNTXentLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'NTXentLoss'"
     ]
    }
   ],
   "source": [
    "ml = losses.NTXentLoss(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = torch.cat((targ, rand_aug), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.arange(targ.shape[0]).repeat(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       " tensor([4, 5, 6, 7, 0, 1, 2, 3]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n",
       "         4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7]),\n",
       " tensor([1, 2, 3, 5, 6, 7, 0, 2, 3, 4, 6, 7, 0, 1, 3, 4, 5, 7, 0, 1, 2, 4, 5, 6,\n",
       "         1, 2, 3, 5, 6, 7, 0, 2, 3, 4, 6, 7, 0, 1, 3, 4, 5, 7, 0, 1, 2, 4, 5, 6]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_pairs_indices(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pytorch_metric_learning import losses\n",
    "class XentLoss(losses.NTXentLoss):\n",
    "    def forward(self, output1, output2):\n",
    "        stacked = torch.cat((output1, output2), dim=0)\n",
    "        labels = torch.arange(output1.shape[0]).repeat(2)\n",
    "        return super().forward(stacked, labels, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8078)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml(stacked, labels, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it into code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# https://arxiv.org/pdf/2002.05709.pdf\n",
    "class XentContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temp=0.5):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "\n",
    "    def forward(self, output1, output2, labels):\n",
    "        cos_sim = F.cosine_similarity(output1, output2, dim=-1)/self.temp\n",
    "        cexp = torch.exp(cos_sim)\n",
    "        neg_denom = (cexp*(1-labels)).sum(dim=-1)\n",
    "        lsoft = torch.log(cexp/neg_denom)\n",
    "        lsoft = torch.sum(-labels * lsoft, dim=-1)\n",
    "        print(lsoft)\n",
    "        return lsoft.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class XentContrastiveLoss2(nn.Module):\n",
    "    def __init__(self, temp=0.5):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "\n",
    "    def forward(self, output1, output2, labels):\n",
    "        cos_sim = F.cosine_similarity(output1, output2, dim=-1)/self.temp\n",
    "        cexp = torch.exp(cos_sim)\n",
    "        x = (cexp * labels).sum(dim=-1)\n",
    "        denom = cexp.sum(dim=-1) - x\n",
    "        lsoft = -torch.log(x/denom)\n",
    "        print(lsoft)\n",
    "        return lsoft.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BatchContrastiveLoss(nn.Module):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "        self.onehot = not isinstance(loss_func, XentOldContrastiveLoss)\n",
    "        \n",
    "    def forward(self, output1, output2):\n",
    "        output1, output2, labels = batched_labels(output1, output2, self.onehot)\n",
    "        return self.loss_func(output1, output2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0.1\n",
    "ls2 = {\n",
    "#     '1_contloss': ContrastiveLoss(margin=1.0),\n",
    "    '4_xentloss': XentContrastiveLoss(temp=temp),\n",
    "    '5_xentloss2': XentContrastiveLoss2(temp=temp),\n",
    "    '6_oldxentloss': XentOldContrastiveLoss(temp=temp)\n",
    "}\n",
    "\n",
    "def batch_loss_out(t,a):\n",
    "    return {k:BatchContrastiveLoss(v)(t,a) for k,v in ls2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,feat=16,128 # \n",
    "targ = torch.randn(bs, feat) # bs x features\n",
    "rand_aug = torch.randn(bs, feat) # bs x features\n",
    "aug = targ + 0.05 # bs x feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4_xentloss': tensor(-6.8875),\n",
       " '5_xentloss2': tensor(-6.8876),\n",
       " '6_oldxentloss': tensor(0.0010)}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss_out(targ,targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4_xentloss': tensor(3.4285),\n",
       " '5_xentloss2': tensor(3.4285),\n",
       " '6_oldxentloss': tensor(3.4732)}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss_out(targ,rand_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4_xentloss': tensor(7.5412),\n",
       " '5_xentloss2': tensor(7.5412),\n",
       " '6_oldxentloss': tensor(7.5418)}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs,feat=256,256 # \n",
    "targ = torch.randn(bs, feat) # bs x features\n",
    "aug = targ+0.1 # bs x features\n",
    "rand_aug = torch.randn(bs, feat) # bs x features\n",
    "batch_loss_out(targ,-aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_metadata.ipynb.\n",
      "Converted 01_preprocess.ipynb.\n",
      "Converted 01_preprocess_mean_std.ipynb.\n",
      "Converted 02_train.ipynb.\n",
      "Converted 02_train_01_save_features.ipynb.\n",
      "Converted 03_train3d.ipynb.\n",
      "Converted 04_trainfull3d_deprecated.ipynb.\n",
      "Converted 04_trainfull3d_labels.ipynb.\n",
      "Converted 05_train_adjacent.ipynb.\n",
      "Converted 06_seutao_features.ipynb.\n",
      "Converted 07_adni.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "#\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "Converted 07_adni_01.ipynb.\n",
      "Converted 08_contrastive_loss-Copy1.ipynb.\n",
      "Converted 08_contrastive_loss.ipynb.\n",
      "Converted 08_imagewang.ipynb.\n",
      "Converted 08_train_self_supervised.ipynb.\n",
      "Converted 08_train_self_supervised_train_1.ipynb.\n",
      "Converted 08_train_self_supervised_train_2_nocombined.ipynb.\n",
      "Converted 08_train_self_supervised_train_2_nocombined_contrast.ipynb.\n",
      "Converted 08_train_self_supervised_train_3.ipynb.\n",
      "Converted 08_train_self_supervised_train_4_nocombined_xent-Copy1.ipynb.\n",
      "Converted 08_train_self_supervised_train_4_nocombined_xent.ipynb.\n",
      "Converted 09_ImageWang_Leadboard_128.ipynb.\n",
      "Converted 09_ImageWang_Leadboard_SS.ipynb.\n",
      "Converted 09_ImageWang_Leadboard_SS_1.ipynb.\n",
      "Converted 09_ImageWang_Leadboard_SS_2.ipynb.\n",
      "Converted Tabular_02_FeatureImportance.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted Untitled1.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
