{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading imports\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from rsna_retro.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss \n",
    "\n",
    "### Taken from here: https://github.com/adambielski/siamese-triplet/blob/master/losses.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss\n",
    "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative=None, size_average=True):\n",
    "        if negative is None: negative = positive.flip(dims=[0])\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checking Triple loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tloss = TripletLoss(margin=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(19.5808))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anch = torch.randn(4, 8) # bs x features\n",
    "pos = anch + 0.1 # bs x feat\n",
    "neg = pos.flip(dims=[0])\n",
    "tloss(anch,pos,neg), tloss(anch,neg,pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tloss(anch,pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# https://github.com/adambielski/siamese-triplet/blob/master/losses.py\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss\n",
    "    Takes embeddings of two samples and a target label == 1 if samples are from the same class and label == 0 otherwise\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, output1, output2, target, size_average=True):\n",
    "        distances = (output2 - output1).pow(2).sum(-1)  # squared distances\n",
    "#         distances = F.pairwise_distance(output1, output2, keepdim=True).pow(2).sum(-1)  # squared distances\n",
    "        losses = (target.float() * distances +\n",
    "                        (1 + -1 * target).float() * F.relu(self.margin - (distances + self.eps).sqrt()).pow(2))\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code\n",
    "# https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch/blob/master/Siamese-networks-medium.ipynb\n",
    "class PairContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = False)\n",
    "        if len(euclidean_distance.shape)>1: euclidean_distance = euclidean_distance.sum(-1)\n",
    "        c_pos = (label) * torch.pow(euclidean_distance, 2)\n",
    "        c_neg = (1-label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        loss_contrastive = (c_neg + c_pos)\n",
    "        return loss_contrastive.mean()\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# https://stackoverflow.com/questions/47107589/how-do-i-use-a-bytetensor-in-a-contrastive-cosine-loss-function\n",
    "class CosineContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        cos_sim = F.cosine_similarity(output1, output2, dim=-1)\n",
    "        loss_cos_con = torch.mean((label) * torch.div(torch.pow((1.0-cos_sim), 2), 4) +\n",
    "                                    (1-label) * torch.pow(cos_sim * torch.lt(cos_sim, self.margin), 2))\n",
    "        return loss_cos_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(output1, output2):\n",
    "    num = output1.T @ output2\n",
    "    denom = torch.norm(output1) * torch.norm(output2)\n",
    "    out = num / denom\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = {\n",
    "    '1_contloss': ContrastiveLoss(),\n",
    "    '2_pairloss': PairContrastiveLoss(),\n",
    "    '3_cosloss': CosineContrastiveLoss(),\n",
    "#     '4_xentloss': XentContrastiveLoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_out(t,a,l):\n",
    "    return {k:v(t,a,l) for k,v in ls.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8]), torch.Size([4, 8]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ = torch.randn(4, 8) # bs x features\n",
    "aug = targ + 0.1 # bs x feat\n",
    "targ.shape, aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_contloss': tensor(0.0200),\n",
       " '2_pairloss': tensor(0.0200),\n",
       " '3_cosloss': tensor(0.0675)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.zeros(aug.shape[0])\n",
    "labels[0] = 1\n",
    "\n",
    "# single target to rest of batch\n",
    "loss_out(targ[:1], aug, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_contloss': tensor(0.0800),\n",
       " '2_pairloss': tensor(0.0800),\n",
       " '3_cosloss': tensor(1.2546e-05)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-to-1 match targ -> aug. Loss should be 8x higher\n",
    "labels = torch.ones(aug.shape[0])\n",
    "loss_out(targ, aug, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_contloss': tensor(11.2569),\n",
       " '2_pairloss': tensor(11.2569),\n",
       " '3_cosloss': tensor(0.1279)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single targ -> all rand. Super High Loss\n",
    "labels = torch.ones(aug.shape[0])\n",
    "loss_out(targ[:1], aug, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batched contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def batched_labels(output1, output2, onehot=True):\n",
    "    bs = output1.shape[0]\n",
    "    rp = [1]*len(output1.shape)\n",
    "    o1 = output1.repeat(*rp,bs).view(bs,*output1.shape)\n",
    "    labels = torch.arange(bs, device=output1.device)\n",
    "    if onehot: labels = torch.eye(o1.shape[0], device=output1.device)[labels]\n",
    "    return o1, output2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "closs = ContrastiveLoss()\n",
    "# closs = CosineContrastiveLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,feat=6,18 # \n",
    "targ = torch.randn(bs, feat)*5 # bs x features\n",
    "aug = targ+0.1 # bs x features\n",
    "# aug = torch.randn(bs, feat) # bs x features\n",
    "o1, o2, l = batched_labels(targ, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6, 18])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1.pow(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6, 18])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(120.5008)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(o1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6, 18]), torch.Size([6, 18]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1.shape, o2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0006)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = F.cosine_similarity(o1, o2, dim=-1)/0.1\n",
    "F.cross_entropy(cs, torch.arange(bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7311, 0.2689],\n",
       "        [0.2689, 0.7311]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.eye(2), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3133)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(torch.eye(2), torch.tensor([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20.0000, -2.7058, -4.8569, -4.7018, -3.6809, -0.8319],\n",
       "        [-2.7058, 20.0000,  6.4985, -8.4361,  3.9806,  1.9830],\n",
       "        [-4.8569,  6.4985, 20.0000, -2.4141,  0.0218, -2.2959],\n",
       "        [-4.7018, -8.4361, -2.4141, 20.0000, -1.7941,  0.4546],\n",
       "        [-3.6809,  3.9806,  0.0218, -1.7941, 20.0000,  1.2681],\n",
       "        [-0.8319,  1.9830, -2.2959,  0.4546,  1.2681, 20.0000]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs/0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 1.0000e+00, 4.8068e-30, 0.0000e+00, 1.6383e-35, 7.5278e-40],\n",
       "        [0.0000e+00, 4.8068e-30, 1.0000e+00, 0.0000e+00, 4.2039e-44, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 3.6154e-43],\n",
       "        [0.0000e+00, 1.6383e-35, 4.2039e-44, 0.0000e+00, 1.0000e+00, 2.1091e-41],\n",
       "        [0.0000e+00, 7.5278e-40, 0.0000e+00, 3.6154e-43, 2.1091e-41, 1.0000e+00]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(cs/0.1, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6, 18]), torch.Size([6, 18]), torch.Size([6, 6]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1.shape, o2.shape, l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = []\n",
    "for i in range(targ.shape[0]):\n",
    "    bs = targ.shape[0]\n",
    "    labels = torch.zeros(bs)\n",
    "    labels[i] = 1 # set current target as the only positive label\n",
    "    losses.append(closs(targ[i], aug, labels))\n",
    "torch.stack(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closs(o1,o2,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(130.9190)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checking - setting wrong target as positive label\n",
    "losses = []\n",
    "for i in range(targ.shape[0]):\n",
    "    bs = targ.shape[0]\n",
    "    labels = torch.zeros(bs)\n",
    "    labels[0] = 1 # set current target as the only positive label\n",
    "    losses.append(closs(targ[i], aug, labels))\n",
    "torch.stack(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(131.0263)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l3 = l[torch.zeros(o1.shape[0]).long()]\n",
    "closs(o1,o2,l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(711.8366)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checking - setting wrong target as positive label\n",
    "losses = []\n",
    "for i in range(targ.shape[0]):\n",
    "    bs = targ.shape[0]\n",
    "    labels = torch.ones(bs)\n",
    "    labels[0] = 1 # set current target as the only positive label\n",
    "    losses.append(closs(targ[i], aug, labels))\n",
    "torch.stack(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(711.6565)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l4 = torch.ones(o1.shape[0], o1.shape[0])\n",
    "closs(o1,o2,l4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XentContrastiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# https://arxiv.org/pdf/2002.05709.pdf\n",
    "class XentOldContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temp=0.5):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "\n",
    "    def forward(self, output1, output2, labels):\n",
    "        cos_sim = F.cosine_similarity(output1, output2, dim=-1)/self.temp\n",
    "        xent_loss = F.cross_entropy(cos_sim,labels.long())\n",
    "        return xent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,feat=4,16 # \n",
    "targ = torch.randn(bs, feat) # bs x features\n",
    "rand_aug = torch.randn(bs, feat) # bs x features\n",
    "aug = targ + 0.1 # bs x feat\n",
    "output1, output2, labels = batched_labels(targ, aug, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6404)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, lmax = labels.max(dim=-1); lmax\n",
    "xent_loss = XentOldContrastiveLoss(1.0)\n",
    "xent_loss(output1, output2, lmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = F.cosine_similarity(output1, output2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6416, -2.1104, -1.7602, -1.7130],\n",
       "        [-2.0705, -0.6104, -2.0386, -1.6067],\n",
       "        [-1.7041, -2.0188, -0.6079, -1.9607],\n",
       "        [-1.7289, -1.6314, -2.0316, -0.7017]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsoft = F.log_softmax(cos_sim, dim=-1); lsoft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6416, -2.0763, -1.7247, -1.7728],\n",
       "        [-2.1046, -0.6104, -2.0372, -1.7007],\n",
       "        [-1.7395, -2.0202, -0.6079, -2.0561],\n",
       "        [-1.6691, -1.5374, -1.9363, -0.7017]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsoft1 = torch.log(torch.exp(cos_sim)/torch.exp(cos_sim).sum(dim=-1)); lsoft1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6404)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nll loss\n",
    "l_nll = torch.mean(torch.sum(-labels * lsoft1, dim=1)); l_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6404)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.nll_loss(lsoft, lmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Xent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9977, -0.4712, -0.1209, -0.0737],\n",
       "        [-0.4654,  0.9947, -0.4335, -0.0016],\n",
       "        [-0.1003, -0.4151,  0.9959, -0.3569],\n",
       "        [-0.0298,  0.0677, -0.3325,  0.9975]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7120, 0.6243, 0.8861, 0.9289],\n",
       "        [0.6279, 2.7039, 0.6483, 0.9984],\n",
       "        [0.9046, 0.6603, 2.7070, 0.6998],\n",
       "        [0.9706, 1.0701, 0.7171, 2.7114]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cexp = torch.exp(cos_sim)\n",
    "cexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.1513, 4.9784, 4.9718, 5.4692])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cexp.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4393, 2.2746, 2.2647, 2.7578])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_denom = (cexp*(1-labels)).sum(dim=-1); neg_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9977, 0.9947, 0.9959, 0.9975])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = cos_sim[range(lmax.shape[0]),lmax]; pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4393, 2.2746, 2.2647, 2.7578])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cexp.sum(dim=-1) - torch.exp(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1060, -1.2930, -0.9384, -1.0882],\n",
       "        [-1.3571,  0.1729, -1.2509, -1.0160],\n",
       "        [-0.9920, -1.2368,  0.1784, -1.3714],\n",
       "        [-0.9215, -0.7541, -1.1499, -0.0170]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsoft2 = torch.log(cexp/neg_denom); lsoft2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.9776)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_nll = torch.mean(torch.sum(-lmax * lsoft2, dim=1)); l_nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7120, 0.6243, 0.8861, 0.9289],\n",
       "        [0.6279, 2.7039, 0.6483, 0.9984],\n",
       "        [0.9046, 0.6603, 2.7070, 0.6998],\n",
       "        [0.9706, 1.0701, 0.7171, 2.7114]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = 1.0\n",
    "cos_sim = F.cosine_similarity(output1, output2, dim=-1)/temp\n",
    "cexp = torch.exp(cos_sim)\n",
    "cexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7120, 2.7039, 2.7070, 2.7114])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (cexp * labels).sum(dim=-1); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4393, 2.2746, 2.2647, 2.7578])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cexp*(1-labels)).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4393, 2.2746, 2.2647, 2.7578])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denom = cexp.sum(dim=-1) - x; denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1060,  1.2930,  0.9384,  1.0882],\n",
       "        [ 1.3571, -0.1729,  1.2509,  1.0160],\n",
       "        [ 0.9920,  1.2368, -0.1784,  1.3714],\n",
       "        [ 0.9215,  0.7541,  1.1499,  0.0170]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsoft = -torch.log(cexp/denom)\n",
    "lsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1060, -0.1729, -0.1784,  0.0170])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(x)+torch.log(denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1060, -0.1729, -0.1784,  0.0170])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_denom = (cexp*(1-labels)).sum(dim=-1); neg_denom\n",
    "lsoft1 = torch.log(cexp/neg_denom)\n",
    "lsoft2 = torch.sum(-labels * lsoft1, dim=-1)\n",
    "lsoft2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sim_mat(x, y=None):\n",
    "    \"\"\"\n",
    "    returns a matrix where entry (i,j) is the dot product of x[i] and x[j]\n",
    "    \"\"\"\n",
    "    if y is None:\n",
    "        y = x\n",
    "    return torch.matmul(x, y.t())\n",
    "\n",
    "def convert_to_pairs(indices_tuple, labels):\n",
    "    \"\"\"\n",
    "    This returns anchor-positive and anchor-negative indices,\n",
    "    regardless of what the input indices_tuple is\n",
    "    Args:\n",
    "        indices_tuple: tuple of tensors. Each tensor is 1d and specifies indices\n",
    "                        within a batch\n",
    "        labels: a tensor which has the label for each element in a batch\n",
    "    \"\"\"\n",
    "    if indices_tuple is None:\n",
    "        return get_all_pairs_indices(labels)\n",
    "    elif len(indices_tuple) == 4:\n",
    "        return indices_tuple\n",
    "    else:\n",
    "        a, p, n = indices_tuple\n",
    "        return a, p, a, n\n",
    "\n",
    "def get_all_pairs_indices(labels, ref_labels=None):\n",
    "    \"\"\"\n",
    "    Given a tensor of labels, this will return 4 tensors.\n",
    "    The first 2 tensors are the indices which form all positive pairs\n",
    "    The second 2 tensors are the indices which form all negative pairs\n",
    "    \"\"\"\n",
    "    if ref_labels is None:\n",
    "        ref_labels = labels\n",
    "    labels1 = labels.unsqueeze(1)\n",
    "    labels2 = ref_labels.unsqueeze(0)\n",
    "    matches = (labels1 == labels2).byte()\n",
    "    diffs = matches ^ 1\n",
    "    if ref_labels is labels:\n",
    "        matches -= torch.eye(matches.size(0)).byte().to(labels.device)\n",
    "    a1_idx = matches.nonzero()[:, 0].flatten()\n",
    "    p_idx = matches.nonzero()[:, 1].flatten()\n",
    "    a2_idx = diffs.nonzero()[:, 0].flatten()\n",
    "    n_idx = diffs.nonzero()[:, 1].flatten()\n",
    "    return a1_idx, p_idx, a2_idx, n_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTXentLoss2(nn.Module):\n",
    "\n",
    "    def __init__(self, temperature, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.temperature = temperature\n",
    "        self.normalize_embeddings = True\n",
    "\n",
    "    def forward(self, embeddings, labels, indices_tuple):\n",
    "        cosine_similarity = sim_mat(embeddings)\n",
    "        if not self.normalize_embeddings:\n",
    "            embedding_norms_mat = self.embedding_norms.unsqueeze(0)*self.embedding_norms.unsqueeze(1)\n",
    "            cosine_similarity = cosine_similarity / (embedding_norms_mat)\n",
    "        cosine_similarity = cosine_similarity / self.temperature\n",
    "\n",
    "        a1, p, a2, n = convert_to_pairs(indices_tuple, labels)\n",
    "\n",
    "        if len(a1) > 0 and len(a2) > 0:\n",
    "            pos_pairs = cosine_similarity[a1, p].unsqueeze(1)\n",
    "            neg_pairs = cosine_similarity[a2, n]\n",
    "            n_per_p = (a2.unsqueeze(0) == a1.unsqueeze(1)).float()\n",
    "            neg_pairs = neg_pairs*n_per_p\n",
    "            neg_pairs[n_per_p==0] = float('-inf')\n",
    "\n",
    "            max_val = torch.max(pos_pairs, torch.max(neg_pairs, dim=1, keepdim=True)[0])\n",
    "            numerator = torch.exp(pos_pairs - max_val).squeeze(1)\n",
    "            denominator = torch.sum(torch.exp(neg_pairs - max_val), dim=1) + numerator\n",
    "            log_exp = torch.log((numerator/denominator) + 1e-20)\n",
    "            return torch.mean(-log_exp)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = losses.NTXentLoss(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = torch.cat((targ, rand_aug), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.arange(targ.shape[0]).repeat(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       " tensor([4, 5, 6, 7, 0, 1, 2, 3]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n",
       "         4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7]),\n",
       " tensor([1, 2, 3, 5, 6, 7, 0, 2, 3, 4, 6, 7, 0, 1, 3, 4, 5, 7, 0, 1, 2, 4, 5, 6,\n",
       "         1, 2, 3, 5, 6, 7, 0, 2, 3, 4, 6, 7, 0, 1, 3, 4, 5, 7, 0, 1, 2, 4, 5, 6]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_pairs_indices(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pytorch_metric_learning import losses\n",
    "class XentLoss(losses.NTXentLoss):\n",
    "    def forward(self, output1, output2):\n",
    "        stacked = torch.cat((output1, output2), dim=0)\n",
    "        labels = torch.arange(output1.shape[0]).repeat(2)\n",
    "        return super().forward(stacked, labels, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8078)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml(stacked, labels, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it into code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# https://arxiv.org/pdf/2002.05709.pdf\n",
    "class XentContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temp=0.5):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "\n",
    "    def forward(self, output1, output2, labels):\n",
    "        cos_sim = F.cosine_similarity(output1, output2, dim=-1)/self.temp\n",
    "        cexp = torch.exp(cos_sim)\n",
    "        neg_denom = (cexp*(1-labels)).sum(dim=-1)\n",
    "        lsoft = torch.log(cexp/neg_denom)\n",
    "        lsoft = torch.sum(-labels * lsoft, dim=-1)\n",
    "        print(lsoft)\n",
    "        return lsoft.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class XentContrastiveLoss2(nn.Module):\n",
    "    def __init__(self, temp=0.5):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "\n",
    "    def forward(self, output1, output2, labels):\n",
    "        cos_sim = F.cosine_similarity(output1, output2, dim=-1)/self.temp\n",
    "        cexp = torch.exp(cos_sim)\n",
    "        x = (cexp * labels).sum(dim=-1)\n",
    "        denom = cexp.sum(dim=-1) - x\n",
    "        lsoft = -torch.log(x/denom)\n",
    "        print(lsoft)\n",
    "        return lsoft.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BatchContrastiveLoss(nn.Module):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "        self.onehot = not isinstance(loss_func, XentOldContrastiveLoss)\n",
    "        \n",
    "    def forward(self, output1, output2):\n",
    "        output1, output2, labels = batched_labels(output1, output2, self.onehot)\n",
    "        return self.loss_func(output1, output2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0.1\n",
    "ls2 = {\n",
    "#     '1_contloss': ContrastiveLoss(margin=1.0),\n",
    "    '4_xentloss': XentContrastiveLoss(temp=temp),\n",
    "    '5_xentloss2': XentContrastiveLoss2(temp=temp),\n",
    "    '6_oldxentloss': XentOldContrastiveLoss(temp=temp)\n",
    "}\n",
    "\n",
    "def batch_loss_out(t,a):\n",
    "    return {k:BatchContrastiveLoss(v)(t,a) for k,v in ls2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,feat=16,128 # \n",
    "targ = torch.randn(bs, feat) # bs x features\n",
    "rand_aug = torch.randn(bs, feat) # bs x features\n",
    "aug = targ + 0.05 # bs x feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4_xentloss': tensor(-6.8875),\n",
       " '5_xentloss2': tensor(-6.8876),\n",
       " '6_oldxentloss': tensor(0.0010)}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss_out(targ,targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4_xentloss': tensor(3.4285),\n",
       " '5_xentloss2': tensor(3.4285),\n",
       " '6_oldxentloss': tensor(3.4732)}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss_out(targ,rand_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4_xentloss': tensor(7.5412),\n",
       " '5_xentloss2': tensor(7.5412),\n",
       " '6_oldxentloss': tensor(7.5418)}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs,feat=256,256 # \n",
    "targ = torch.randn(bs, feat) # bs x features\n",
    "aug = targ+0.1 # bs x features\n",
    "rand_aug = torch.randn(bs, feat) # bs x features\n",
    "batch_loss_out(targ,-aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_metadata.ipynb.\n",
      "Converted 01_preprocess.ipynb.\n",
      "Converted 01_preprocess_mean_std.ipynb.\n",
      "Converted 02_train.ipynb.\n",
      "Converted 02_train_01_save_features.ipynb.\n",
      "Converted 03_train3d.ipynb.\n",
      "Converted 04_trainfull3d_deprecated.ipynb.\n",
      "Converted 04_trainfull3d_labels.ipynb.\n",
      "Converted 05_train_adjacent.ipynb.\n",
      "Converted 06_seutao_features.ipynb.\n",
      "Converted 07_adni.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "#\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "Converted 07_adni_01.ipynb.\n",
      "Converted 08_contrastive_loss-Copy1.ipynb.\n",
      "Converted 08_contrastive_loss.ipynb.\n",
      "Converted 08_imagewang.ipynb.\n",
      "Converted 08_train_self_supervised.ipynb.\n",
      "Converted 08_train_self_supervised_train_1.ipynb.\n",
      "Converted 08_train_self_supervised_train_2_nocombined.ipynb.\n",
      "Converted 08_train_self_supervised_train_2_nocombined_contrast.ipynb.\n",
      "Converted 08_train_self_supervised_train_3.ipynb.\n",
      "Converted 08_train_self_supervised_train_4_nocombined_xent-Copy1.ipynb.\n",
      "Converted 08_train_self_supervised_train_4_nocombined_xent.ipynb.\n",
      "Converted 09_ImageWang_Leadboard_128.ipynb.\n",
      "Converted 09_ImageWang_Leadboard_SS.ipynb.\n",
      "Converted 09_ImageWang_Leadboard_SS_1.ipynb.\n",
      "Converted 09_ImageWang_Leadboard_SS_2.ipynb.\n",
      "Converted Tabular_02_FeatureImportance.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted Untitled1.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
