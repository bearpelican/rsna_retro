{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading imports\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ashaw/.kaggle/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from rsna_retro.imports import *\n",
    "from rsna_retro.metadata import *\n",
    "from rsna_retro.preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_pil_fn(p):\n",
    "    def _f(fn): return PILCTScan.create(p/f'{fn}.jpg')\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fn2label(fn): return Meta.df_comb.loc[fn][htypes].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "mean = [x/255.0 for x in [ 56.2214,  62.1220, 141.9133]]\n",
    "std = [x/255.0 for x in [93.0132, 77.8876, 50.8730]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_wgts(df, splits):\n",
    "    wgts = df['any'][splits[0]].values\n",
    "    return wgts * (1/0.14 - 2) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_data_gen(fns, bs, img_tfm, splits, sz=None, nw=8, mean=mean, std=std,\n",
    "        wgts=None, batch_xtra=None, after_item=None, with_aug=True, test=False, **kwargs):\n",
    "    tfms = [[img_tfm, ToTensor], [fn2label,EncodedMultiCategorize(htypes)]]\n",
    "    if test: tfms = [tfms[0]]\n",
    "    dsrc = DataSource(fns, tfms, splits=splits)\n",
    "    nrm = Normalize.from_stats(mean,std)\n",
    "    batch_tfms = L(IntToFloatTensor, nrm, Cuda()) + L(batch_xtra)\n",
    "    if with_aug: batch_tfms += aug_transforms(**kwargs)\n",
    "    if sz is not None:\n",
    "        batch_tfms = batch_tfms+[RandomResizedCropGPU(sz, min_scale=0.7, ratio=(1.,1.), valid_scale=0.9)]\n",
    "    if wgts is None:\n",
    "        return dsrc.databunch(bs=bs, num_workers=nw, after_item=after_item, after_batch=batch_tfms)\n",
    "    else:\n",
    "        return dsrc.weighted_databunch(wgts, bs=bs, num_workers=nw, after_item=after_item, after_batch=batch_tfms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def filename(o): return os.path.splitext(os.path.basename(o))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_data(bs, sz, img_dir='nocrop_jpg', full_ds=False):\n",
    "    if sz is None or sz <= 256: img_dir = f'{img_dir}256'\n",
    "    df,splits = (Meta.df_comb,Meta.splits) if full_ds else (Meta.df_any,Meta.splits_any)\n",
    "    return get_data_gen(L(list(df.index)), bs=bs, img_tfm=get_pil_fn(path/img_dir), \n",
    "                        sz=sz, splits=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def accuracy_any(inp, targ, thresh=0.5, sigmoid=True):\n",
    "    inp,targ = flatten_check(inp[:,0],targ[:,0])\n",
    "    if sigmoid: inp = inp.sigmoid()\n",
    "    return ((inp>thresh)==targ.bool()).float().mean()\n",
    "\n",
    "\n",
    "def get_loss(scale=None):\n",
    "    num_classes = 6\n",
    "    loss_weights = to_device(tensor(2.0, 1, 1, 1, 1, 1))\n",
    "    loss_weights = loss_weights/loss_weights.sum()*num_classes\n",
    "    \n",
    "    if scale is not None: scale = to_device(tensor([scale]*num_classes))\n",
    "    return BaseLoss(nn.BCEWithLogitsLoss, weight=loss_weights, #pos_weight=scale,\n",
    "                    floatify=True, flatten=False, is_2d=False, activation=torch.sigmoid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_learner(dbch, arch_or_model, lf=None, pretrained=False, opt_func=None, metrics=None, fp16=True, config=None):\n",
    "    if lf is None: lf = get_loss()\n",
    "    if metrics is None: metrics=[accuracy_multi,accuracy_any]\n",
    "    if opt_func is None: opt_func = partial(Adam, wd=1e-5, eps=1e-4, sqr_mom=0.999)\n",
    "    if isinstance(arch_or_model, nn.Module):\n",
    "        learn = Learner(dbch, arch_or_model, loss_func=lf, lr=3e-3,\n",
    "                    opt_func=opt_func, metrics=metrics)\n",
    "    else:\n",
    "        if config is None: config=dict(ps=0., lin_ftrs=[], concat_pool=False)\n",
    "        learn = cnn_learner(dbch, arch_or_model, pretrained=pretrained, loss_func=lf, lr=3e-3,\n",
    "                            opt_func=opt_func, metrics=metrics, config=config)\n",
    "    return learn.to_fp16() if fp16 else learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_tune(bs, sz, epochs, lr):\n",
    "#     learn.dbunch = get_data(bs, sz)\n",
    "#     do_fit(learn, epochs, lr, freeze=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def do_fit(learn, epochs, lr, freeze=False, do_slice=False, **kwargs):\n",
    "    if do_slice: lr = slice(lr*3,lr)\n",
    "    if freeze:\n",
    "        learn.freeze()\n",
    "        learn.fit_one_cycle(1, lr, div=2, div_final=1, pct_start=0.1)\n",
    "    learn.unfreeze()\n",
    "    learn.fit_one_cycle(epochs, lr, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_test_data(df_tst, bs=512, sz=256, tst_dir='tst_jpg'):\n",
    "    tst_fns = df_tst.index.values\n",
    "    tst_splits = [L.range(tst_fns), L.range(tst_fns)]\n",
    "    tst_dbch = get_data_gen(tst_fns, bs=bs, img_tfm=get_pil_fn(path/tst_dir), sz=sz, splits=tst_splits, test=True)\n",
    "    tst_dbch.c = 6\n",
    "    return tst_dbch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def submission(df_tst, preds, fn='submission'):\n",
    "    ids,labels = [],[]\n",
    "    for idx,pred in zip(df_tst.index, preds):\n",
    "        for i,label in enumerate(htypes):\n",
    "            ids.append(f\"{idx}_{label}\")\n",
    "            labels.append('{0:1.10f}'.format(pred[i].item()))\n",
    "    df_csv = pd.DataFrame({'ID': ids, 'Label': labels})\n",
    "    df_csv.to_csv(f'{fn}.csv', index=False)\n",
    "    return df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbch = get_data(512, 128)\n",
    "learn = get_learner(dbch, xresnet34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_fit(learn, 1, 1e-2)\n",
    "learn.save(f'runs/{name}-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai2.learner.Learner at 0x7f0d978ed950>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load(f'runs/{name}-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.dbunch = get_test_data(Meta.df_tst, bs=256, sz=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tst = test_dl(learn.dbunch, tst_fns, after_item=[get_pil_fn(path/'tst_jpg'), ToTensor])\n",
    "\n",
    "# tst.tfms = Pipeline(funcs=[get_pil_fn(path/'tst_jpg'), ToTensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_fn = f'subm/{name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,targs = learn.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_csv = submission(df_tst, preds, fn=sub_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FileLink(f'{sub_fn}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.0M/26.0M [00:03<00:00, 8.99MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Successfully submitted to RSNA Intracranial Hemorrhage Detection"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.competition_submit(f'{nm}.csv', 'testing rsna_retro submission', 'rsna-intracranial-hemorrhage-detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ref': 13695014,\n",
       " 'totalBytes': 27277209,\n",
       " 'date': '2019-12-08T22:49:07.013Z',\n",
       " 'description': 'testing rsna_retro submission',\n",
       " 'errorDescription': None,\n",
       " 'fileName': 'test_sub.csv',\n",
       " 'publicScore': '0.95475',\n",
       " 'privateScore': '0.06444',\n",
       " 'status': 'complete',\n",
       " 'submittedBy': 'Andrew Shaw',\n",
       " 'submittedByRef': 'bearpelican',\n",
       " 'teamName': 'Andrew Shaw',\n",
       " 'type': 'standard',\n",
       " 'url': 'https://www.kaggle.com/submissions/13695014/13695014.raw'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.competitions_submissions_list('rsna-intracranial-hemorrhage-detection')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DummyLoss:\n",
    "    def __call__(self, p, *t, **kwargs): return torch.tensor(0, device=p.device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_features(learn, feat_path):\n",
    "    preds,targs = learn.get_preds(dl=dbunch.valid_dl)\n",
    "    val_ids = dbunch.valid_dl.dataset.items\n",
    "    feat_path.mkdir(exist_ok=True)\n",
    "    for idx,pred in progress_bar(zip(val_ids, preds), total=len(val_ids)):\n",
    "        np.save(str(feat_path/f'{idx}'), pred.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_save = 'runs/baseline_any-3-full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "path_feat256 = path/'features_256'\n",
    "path_feat256_tst = path/'tst_features_256'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = get_test_data(Meta.df_comb, bs=512, sz=None, tst_dir='nocrop_jpg256')\n",
    "learn = get_learner(dbunch, xresnet34, lf=DummyLoss(), metrics=[])\n",
    "learn.load(fn_save)\n",
    "learn.model = learn.model[0]\n",
    "save_features(learn, path_feat256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = get_test_data(Meta.df_tst, bs=512, sz=None, tst_dir='tst_jpg256')\n",
    "learn = get_learner(dbunch, xresnet34, lf=DummyLoss(), metrics=[])\n",
    "learn.load(fn_save)\n",
    "learn.model = learn.model[0]\n",
    "save_features(learn, path_feat256_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_metadata.ipynb.\n",
      "Converted 01_preprocess.ipynb.\n",
      "Converted 02_train.ipynb.\n",
      "Converted 02_train_01_full.ipynb.\n",
      "Converted 02_train_01_full_256.ipynb.\n",
      "Converted 02_train_02_any_resnet18.ipynb.\n",
      "Converted 02_train_02_any_xresnet18.ipynb.\n",
      "Converted 02_train_02_any_xresnet34.ipynb.\n",
      "Converted 02_train_02c_any_old_double_avg_pool.ipynb.\n",
      "Converted 03_train3d.ipynb.\n",
      "Converted 03_train3d_00_train3d.ipynb.\n",
      "Converted 03_train3d_01_train3d-Copy1.ipynb.\n",
      "Converted 03_train3d_01_train3d-resnet18.ipynb.\n",
      "Converted 03_train3d_01_train3d.ipynb.\n",
      "Converted 03_train3d_02_train_head.ipynb.\n",
      "Converted 04_trainSeq_01_lstm.ipynb.\n",
      "Converted 04_trainSeq_02_transformer.ipynb.\n",
      "Converted 05_train_slice.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
